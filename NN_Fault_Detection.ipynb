{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C-PuW8TYfTxB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, Y = None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "      return self.X[index] , self.Y[index]\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) #same amount of data points"
   ],
   "metadata": {
    "id": "HMTiCkbpgVoV"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#load the files google colab mode\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "path_drive = r'drive/MyDrive/Fault Detection/'\n",
    "\n",
    "!unzip drive/MyDrive/Fault_Detection/Data_diff\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbVK1ZTSs3md",
    "outputId": "0434c7d3-29d5-45c7-cf69-d6676cd6c3d0"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n",
      "Archive:  drive/MyDrive/Fault_Detection/Data_diff.zip\n",
      "  inflating: Features.npy            \n",
      "  inflating: Features_fine.npy       \n",
      "  inflating: Features_val.npy        \n",
      "  inflating: Target.npy              \n",
      "  inflating: Target_fine.npy         \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = np.load(r'Features.npy')\n",
    "Y_train = np.load(r'Target.npy')\n",
    "\n",
    "X_fine = np.load(r'Features_fine.npy')\n",
    "Y_fine = np.load(r'Target_fine.npy')\n",
    "\n",
    "X_train = np.concatenate((X_train, X_fine, X_fine, X_fine, X_fine, X_fine, X_fine, X_fine, X_fine), axis = 0)\n",
    "Y_train = np.concatenate((Y_train, Y_fine, Y_fine, Y_fine, Y_fine, Y_fine, Y_fine, Y_fine, Y_fine), axis = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ],
   "metadata": {
    "id": "JZuvAdA_gEJa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fed149d9-2f49-43fd-df6f-1e8c4d00b78e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Features.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFeatures.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m Y_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTarget.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m X_fine \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFeatures_fine.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Anaconda3\\anaconda\\envs\\DS\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    403\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 405\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    406\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Features.npy'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    source : - https://gist.github.com/EdisonLeeeee/803c2f91effa9f3fd4e1b3f4870d9842\n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. 0 <= val <= 1\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    '''\n",
    "    assert y_true.ndim == 1\n",
    "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
    "\n",
    "    if y_pred.ndim == 2:\n",
    "        y_pred = y_pred.argmax(dim=1)\n",
    "\n",
    "\n",
    "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
    "    #f1.requires_grad = is_training\n",
    "    return 1-f1"
   ],
   "metadata": {
    "id": "onQBCq70-pd1"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "dataset = Data(X_train, Y_train)\n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "amount_train = 0.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_data_size = len(X_train)\n",
    "train_size = int(amount_train*training_data_size)\n",
    "test_size = training_data_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "data_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "data_test = DataLoader(test_dataset, batch_size=batch_size, shuffle =False)"
   ],
   "metadata": {
    "id": "pYU6SG8ghcJ1"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class NN(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(NN, self).__init__()\n",
    "\n",
    "      self.lc1 = nn.Linear(40, 512)\n",
    "      self.lc2 = nn.Linear(512, 1024)\n",
    "      self.lc3 = nn.Linear(1024, 256)\n",
    "      self.lc4 = nn.Linear(256, 1)\n",
    "\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, X):\n",
    "\n",
    "      X = X.view(-1,40)\n",
    "\n",
    "      X = F.relu(self.lc1(X))\n",
    "      X = self.dropout(X)\n",
    "\n",
    "      X = F.relu(self.lc2(X))\n",
    "      X = self.dropout(X)\n",
    "\n",
    "      X = F.relu(self.lc3(X))\n",
    "      X = self.dropout(X)\n",
    "\n",
    "      X = torch.sigmoid(self.lc4(X))\n",
    "\n",
    "      X = X.flatten()\n",
    "      return X"
   ],
   "metadata": {
    "id": "2D1DWlUMhuvm"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "DNN = NN().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(DNN.parameters(), lr = learning_rate)\n",
    "epochs = 50\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.96)"
   ],
   "metadata": {
    "id": "ARsii043jZXv"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(epochs, DNN, loss_fn, optimizer,\n",
    "          data_train, data_test, device, scheduler):\n",
    "    print('Starting training..')\n",
    "    for e in range(0, epochs):\n",
    "        print('='*20)\n",
    "        print(f'Starting epoch {e + 1}/{epochs}')\n",
    "        print('='*20)\n",
    "\n",
    "        train_loss = 0.\n",
    "        val_loss = 0.\n",
    "\n",
    "        DNN.train()\n",
    "        # set model to training phase\n",
    "\n",
    "        for train_step, (X ,labels) in enumerate(data_train):\n",
    "\n",
    "            X = X.to(device)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            # set the gradients to zero (you can do so by accessing the optimizer)\n",
    "            optimizer.zero_grad()\n",
    "            # compute outputs\n",
    "            output = DNN(X).to(device)\n",
    "\n",
    "\n",
    "            # compute loss  (you have defined the loss_fn above!)\n",
    "            loss = f1_loss(output, labels, True)\n",
    "\n",
    "            #  use backward() so that the whole graph is differentiated w.r.t. the loss\n",
    "            loss.backward()\n",
    "            # performs a parameter update based on the current gradient (Note: you need to do it through the optimizer)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if e % 2 == 0 and train_step % 8000 == 0 and train_step > 0:\n",
    "\n",
    "                print('Evaluating at step', train_step)\n",
    "\n",
    "                accuracy = 0\n",
    "\n",
    "                f_1 = 0\n",
    "\n",
    "                DNN.eval()          # set model to eval phase\n",
    "\n",
    "                for val_step, (X ,labels) in enumerate(data_test):\n",
    "                    X = X.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = DNN(X).to(device)\n",
    "                             # compute outputs\n",
    "\n",
    "                    loss = f1_loss(outputs, labels, True)\n",
    "                    loss = loss.to(device)\n",
    "                        # compute loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    preds = (outputs > 0.5).float()\n",
    "                    accuracy += sum((preds.cpu() == labels.cpu()).numpy())\n",
    "                    f_1 += f1_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "\n",
    "\n",
    "                val_loss /= (val_step + 1)\n",
    "                accuracy = accuracy/(val_step *64)\n",
    "\n",
    "                f_1  /= (val_step + 1)\n",
    "                print(f'F1 Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f_1:.4f}')\n",
    "\n",
    "                DNN.train()\n",
    "\n",
    "\n",
    "\n",
    "        train_loss /= (train_step + 1)\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "    print('Training complete..')"
   ],
   "metadata": {
    "id": "3BERokaAo2kg"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train(epochs, DNN, loss_fn=None, optimizer=optimizer,\n",
    "      data_train=data_train, data_test=data_test, device = device,\n",
    "      scheduler = scheduler\n",
    "      )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUDb7Umkqg-y",
    "outputId": "d323f570-51ac-4bcd-a54e-969c6e2a078e"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.3168, Accuracy: 0.5700, F1 Score: 0.6834\n",
      "Training Loss: 0.3306\n",
      "====================\n",
      "Starting epoch 2/50\n",
      "====================\n",
      "Training Loss: 0.3103\n",
      "====================\n",
      "Starting epoch 3/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.3030, Accuracy: 0.6134, F1 Score: 0.6969\n",
      "Training Loss: 0.3069\n",
      "====================\n",
      "Starting epoch 4/50\n",
      "====================\n",
      "Training Loss: 0.3046\n",
      "====================\n",
      "Starting epoch 5/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.3005, Accuracy: 0.6239, F1 Score: 0.6995\n",
      "Training Loss: 0.3039\n",
      "====================\n",
      "Starting epoch 6/50\n",
      "====================\n",
      "Training Loss: 0.3030\n",
      "====================\n",
      "Starting epoch 7/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2992, Accuracy: 0.6279, F1 Score: 0.7010\n",
      "Training Loss: 0.3024\n",
      "====================\n",
      "Starting epoch 8/50\n",
      "====================\n",
      "Training Loss: 0.3011\n",
      "====================\n",
      "Starting epoch 9/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2999, Accuracy: 0.6176, F1 Score: 0.7002\n",
      "Training Loss: 0.3006\n",
      "====================\n",
      "Starting epoch 10/50\n",
      "====================\n",
      "Training Loss: 0.3002\n",
      "====================\n",
      "Starting epoch 11/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2979, Accuracy: 0.6311, F1 Score: 0.7021\n",
      "Training Loss: 0.2998\n",
      "====================\n",
      "Starting epoch 12/50\n",
      "====================\n",
      "Training Loss: 0.2993\n",
      "====================\n",
      "Starting epoch 13/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2971, Accuracy: 0.6371, F1 Score: 0.7028\n",
      "Training Loss: 0.2988\n",
      "====================\n",
      "Starting epoch 14/50\n",
      "====================\n",
      "Training Loss: 0.2987\n",
      "====================\n",
      "Starting epoch 15/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2971, Accuracy: 0.6349, F1 Score: 0.7030\n",
      "Training Loss: 0.2979\n",
      "====================\n",
      "Starting epoch 16/50\n",
      "====================\n",
      "Training Loss: 0.2978\n",
      "====================\n",
      "Starting epoch 17/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2967, Accuracy: 0.6348, F1 Score: 0.7033\n",
      "Training Loss: 0.2978\n",
      "====================\n",
      "Starting epoch 18/50\n",
      "====================\n",
      "Training Loss: 0.2973\n",
      "====================\n",
      "Starting epoch 19/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2964, Accuracy: 0.6330, F1 Score: 0.7037\n",
      "Training Loss: 0.2971\n",
      "====================\n",
      "Starting epoch 20/50\n",
      "====================\n",
      "Training Loss: 0.2966\n",
      "====================\n",
      "Starting epoch 21/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2961, Accuracy: 0.6383, F1 Score: 0.7041\n",
      "Training Loss: 0.2965\n",
      "====================\n",
      "Starting epoch 22/50\n",
      "====================\n",
      "Training Loss: 0.2963\n",
      "====================\n",
      "Starting epoch 23/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2953, Accuracy: 0.6404, F1 Score: 0.7047\n",
      "Training Loss: 0.2962\n",
      "====================\n",
      "Starting epoch 24/50\n",
      "====================\n",
      "Training Loss: 0.2957\n",
      "====================\n",
      "Starting epoch 25/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2950, Accuracy: 0.6370, F1 Score: 0.7051\n",
      "Training Loss: 0.2954\n",
      "====================\n",
      "Starting epoch 26/50\n",
      "====================\n",
      "Training Loss: 0.2953\n",
      "====================\n",
      "Starting epoch 27/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2952, Accuracy: 0.6329, F1 Score: 0.7049\n",
      "Training Loss: 0.2952\n",
      "====================\n",
      "Starting epoch 28/50\n",
      "====================\n",
      "Training Loss: 0.2946\n",
      "====================\n",
      "Starting epoch 29/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2943, Accuracy: 0.6414, F1 Score: 0.7056\n",
      "Training Loss: 0.2948\n",
      "====================\n",
      "Starting epoch 30/50\n",
      "====================\n",
      "Training Loss: 0.2944\n",
      "====================\n",
      "Starting epoch 31/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2941, Accuracy: 0.6425, F1 Score: 0.7060\n",
      "Training Loss: 0.2942\n",
      "====================\n",
      "Starting epoch 32/50\n",
      "====================\n",
      "Training Loss: 0.2941\n",
      "====================\n",
      "Starting epoch 33/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2938, Accuracy: 0.6420, F1 Score: 0.7064\n",
      "Training Loss: 0.2935\n",
      "====================\n",
      "Starting epoch 34/50\n",
      "====================\n",
      "Training Loss: 0.2935\n",
      "====================\n",
      "Starting epoch 35/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2936, Accuracy: 0.6408, F1 Score: 0.7064\n",
      "Training Loss: 0.2934\n",
      "====================\n",
      "Starting epoch 36/50\n",
      "====================\n",
      "Training Loss: 0.2935\n",
      "====================\n",
      "Starting epoch 37/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2931, Accuracy: 0.6431, F1 Score: 0.7070\n",
      "Training Loss: 0.2935\n",
      "====================\n",
      "Starting epoch 38/50\n",
      "====================\n",
      "Training Loss: 0.2926\n",
      "====================\n",
      "Starting epoch 39/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2927, Accuracy: 0.6415, F1 Score: 0.7074\n",
      "Training Loss: 0.2929\n",
      "====================\n",
      "Starting epoch 40/50\n",
      "====================\n",
      "Training Loss: 0.2928\n",
      "====================\n",
      "Starting epoch 41/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2926, Accuracy: 0.6428, F1 Score: 0.7074\n",
      "Training Loss: 0.2926\n",
      "====================\n",
      "Starting epoch 42/50\n",
      "====================\n",
      "Training Loss: 0.2925\n",
      "====================\n",
      "Starting epoch 43/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2923, Accuracy: 0.6455, F1 Score: 0.7079\n",
      "Training Loss: 0.2926\n",
      "====================\n",
      "Starting epoch 44/50\n",
      "====================\n",
      "Training Loss: 0.2924\n",
      "====================\n",
      "Starting epoch 45/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2922, Accuracy: 0.6444, F1 Score: 0.7081\n",
      "Training Loss: 0.2920\n",
      "====================\n",
      "Starting epoch 46/50\n",
      "====================\n",
      "Training Loss: 0.2922\n",
      "====================\n",
      "Starting epoch 47/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2922, Accuracy: 0.6436, F1 Score: 0.7081\n",
      "Training Loss: 0.2922\n",
      "====================\n",
      "Starting epoch 48/50\n",
      "====================\n",
      "Training Loss: 0.2920\n",
      "====================\n",
      "Starting epoch 49/50\n",
      "====================\n",
      "Evaluating at step 8000\n",
      "F1 Loss: 0.2917, Accuracy: 0.6450, F1 Score: 0.7086\n",
      "Training Loss: 0.2920\n",
      "====================\n",
      "Starting epoch 50/50\n",
      "====================\n",
      "Training Loss: 0.2915\n",
      "Training complete..\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#save models\n",
    "#torch.save(DNN.state_dict(), r'drive/MyDrive/Fault_Detection/fault_dnn_dict_v9.pt')\n",
    "#torch.save(DNN, r'drive/MyDrive/Fault_Detection/fault_dnn_v9.pt')"
   ],
   "metadata": {
    "id": "jLayirUrDckM"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "###DNN.load_state_dict(torch.load(r'drive/MyDrive/Fault_Detection/fault_dnn_dict.pt'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xomoc2oSC8SM",
    "outputId": "9f0432d8-48b6-4549-d965-17a8f32cc00e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "X_val = np.load('Features_val.npy')\n",
    "\n",
    "Y_val = np.zeros(len(X_val))\n",
    "\n",
    "data_val = Data(X_val, Y_val)\n",
    "\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle =False)\n",
    "\n",
    "DNN.eval()\n",
    "\n",
    "target = np.array([])\n",
    "for step, (X ,labels) in enumerate(val_loader):\n",
    "\n",
    "    X = X.to(device)\n",
    "\n",
    "\n",
    "    outputs = DNN(X).to(device)\n",
    "                             # compute outputs\n",
    "    preds = (outputs > 0.5).float()\n",
    "\n",
    "    target = np.append(target, preds.to('cpu').numpy(), axis = 0)\n",
    "\n",
    "\n",
    "sub_file = pd.read_csv(r'SampleSubmission.csv')\n"
   ],
   "metadata": {
    "id": "Kuw8AlnPnIHR"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sub_file['data_rate_t+1_trend'] = target\n",
    "\n",
    "sub_file.to_csv('sub_9.csv', index = False)"
   ],
   "metadata": {
    "id": "sSFZXh1apS6v"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xcu2hiv4LQI9"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
